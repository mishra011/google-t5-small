{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:08\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-09 15:01:08\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: deploy, train, config, version, func, backend, inference\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:08\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:08\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 't5small/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:08\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'google-t5/t5-small', 'project_name': 't5small', 'data_path': 'argilla/distilabel-capybara-dpo-7k-binarized', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 1024, 'padding': None, 'trainer': 'orpo', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 1, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': 'chatml', 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'chosen', 'rejected_text_column': 'rejected', 'push_to_hub': True, 'username': 'mishra011ai', 'token': '*****', 'unsloth': False}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:14\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_orpo\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mStarting ORPO training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:22\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['source', 'conversation', 'original_response', 'generation_prompt', 'raw_generation_responses', 'new_generations', 'prompt', 'chosen', 'rejected', 'rating_chosen', 'rating_rejected', 'chosen_model', 'rejected_model'],\n",
      "    num_rows: 7563\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:22\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "tokenizer_config.json: 100%|███████████████| 2.32k/2.32k [00:00<00:00, 15.5MB/s]\n",
      "spiece.model: 100%|███████████████████████████| 792k/792k [00:00<00:00, 984kB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.39M/1.39M [00:01<00:00, 1.37MB/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:26\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_data_with_chat_template\u001b[0m:\u001b[36m446\u001b[0m - \u001b[1mApplying chat template\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:26\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_data_with_chat_template\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mFor ORPO/DPO, `prompt` will be extracted from chosen messages\u001b[0m\n",
      "Map: 100%|█████████████████████████| 7563/7563 [00:01<00:00, 4119.98 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:28\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:28\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:28\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:28\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mUsing block size 512\u001b[0m\n",
      "config.json: 100%|█████████████████████████| 1.21k/1.21k [00:00<00:00, 3.42MB/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:29\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m583\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-09 15:01:29\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m625\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:29\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m627\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:29\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-09 15:01:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 43, in train\n",
      "    train_orpo(config)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/train_clm_orpo.py\", line 27, in train\n",
      "    model = utils.get_model(config, tokenizer)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/utils.py\", line 649, in get_model\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 567, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-09 15:01:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mUnrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-09 15:01:30\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 21563\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# google-t5/t5-small\n",
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model google-t5/t5-small \\\n",
    "--data-path argilla/distilabel-capybara-dpo-7k-binarized \\\n",
    "--text-column chosen \\\n",
    "--rejected-text-column rejected \\\n",
    "--lr 2e-4 \\\n",
    "--batch-size 2 \\\n",
    "--epochs 1 \\\n",
    "--trainer orpo \\\n",
    "--chat-template chatml \\\n",
    "--peft \\\n",
    "--project-name t5small \\\n",
    "--username mishra011ai \\\n",
    "--push-to-hub \\\n",
    "--token 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: train, backend, config, deploy, inference, version, func\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'sftp-model/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'microsoft/Phi-3-mini-4k-instruct', 'project_name': 'sftp-model', 'data_path': 'argilla/distilabel-capybara-dpo-7k-binarized', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 1, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'chosen', 'rejected_text_column': 'rejected', 'push_to_hub': True, 'username': 'mishra011ai', 'token': '*****', 'unsloth': False}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:39\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:45\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['source', 'conversation', 'original_response', 'generation_prompt', 'raw_generation_responses', 'new_generations', 'prompt', 'chosen', 'rejected', 'rating_chosen', 'rating_rejected', 'chosen_model', 'rejected_model'],\n",
      "    num_rows: 7563\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:45\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m583\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m625\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m627\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:19<00:00,  9.59s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:44:07\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m666\u001b[0m - \u001b[1mmodel dtype: torch.float32\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:44:11\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Generating train split: 0 examples [00:01, ? examples/s]\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-08 11:44:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1748, in _prepare_split_single\n",
      "    for key, record in generator:\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/packaged_modules/generator/generator.py\", line 30, in _generate_examples\n",
      "    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 620, in data_generator\n",
      "    yield from constant_length_iterator\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/utils.py\", line 512, in __iter__\n",
      "    tokenized_inputs = self.tokenizer(buffer, add_special_tokens=self.add_special_tokens, truncation=False)[\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2945, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3004, in _call_one\n",
      "    raise ValueError(\n",
      "ValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 623, in _prepare_packed_dataloader\n",
      "    packed_dataset = Dataset.from_generator(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1125, in from_generator\n",
      "    ).read()\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/io/generator.py\", line 47, in read\n",
      "    self.builder.download_and_prepare(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1027, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1789, in _download_and_prepare\n",
      "    super()._download_and_prepare(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1122, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1627, in _prepare_split\n",
      "    for job_id, done, content in self._prepare_split_single(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1784, in _prepare_split_single\n",
      "    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\n",
      "datasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 28, in train\n",
      "    train_sft(config)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/train_clm_sft.py\", line 44, in train\n",
      "    trainer = SFTTrainer(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 362, in __init__\n",
      "    train_dataset = self._prepare_dataset(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 519, in _prepare_dataset\n",
      "    return self._prepare_packed_dataloader(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 627, in _prepare_packed_dataloader\n",
      "    raise ValueError(\n",
      "ValueError: Error occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence.\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-08 11:44:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mError occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence.\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:44:15\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 48228\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model microsoft/Phi-3-mini-4k-instruct \\\n",
    "--data-path argilla/distilabel-capybara-dpo-7k-binarized \\\n",
    "--text-column chosen \\\n",
    "--rejected-text-column rejected \\\n",
    "--lr 2e-4 \\\n",
    "--batch-size 2 \\\n",
    "--epochs 1 \\\n",
    "--trainer sft \\\n",
    "--peft \\\n",
    "--project-name sftp-model \\\n",
    "--username mishra011ai \\\n",
    "--push-to-hub \\\n",
    "--token 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\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
